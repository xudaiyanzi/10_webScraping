{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step1. Scraping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Scrape the latest News Title and Paragraph Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dependencies\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# import the splinter to browse continuous pages\n",
    "from splinter import Browser\n",
    "\n",
    "# https://splinter.readthedocs.io/en/latest/drivers/chrome.html\n",
    "#!which chromedriver\n",
    "\n",
    "# use the path to start the chrome\n",
    "executable_path = {'executable_path': '/usr/local/bin/chromedriver'}\n",
    "browser = Browser('chrome', **executable_path, headless=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use Splinter with BeautifulSoup\n",
    "url_news = 'https://mars.nasa.gov/news/'\n",
    "browser.visit(url_news)\n",
    "\n",
    "# use beautifulsoup and its html.parser on html\n",
    "html = browser.html\n",
    "soup = BeautifulSoup(html, 'html.parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scrape and store the list\n",
    "time = soup.find('div', class_='list_date').text\n",
    "title = soup.find('div', class_='content_title').text\n",
    "paragraph = soup.find('div', class_='article_teaser_body').text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The lastest news is published at February 28, 2019\n",
      "--------------------------------------\n",
      "Its title: After a Reset, Curiosity Is Operating Normally\n",
      "--------------------------------------\n",
      "It talks about Curiosity has returned to science operations and is once again exploring the clay unit. \n"
     ]
    }
   ],
   "source": [
    "# print the result\n",
    "print(\"The lastest news is published at\", time)\n",
    "print(\"--------------------------------------\")\n",
    "print(\"Its title:\",title)\n",
    "print(\"--------------------------------------\")\n",
    "print(\"It talks about\", paragraph)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Scrape the Featured Images "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dependencies\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# import the splinter to browse continuous pages\n",
    "from splinter import Browser\n",
    "\n",
    "# https://splinter.readthedocs.io/en/latest/drivers/chrome.html\n",
    "!which chromedriver\n",
    "\n",
    "# use the path to start the chrome\n",
    "executable_path = {'executable_path': '/usr/local/bin/chromedriver'}\n",
    "browser = Browser('chrome', **executable_path, headless=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use Splinter with BeautifulSoup\n",
    "url_main = 'https://www.jpl.nasa.gov'\n",
    "url_mars = \"/spaceimages/?search=&category=Mars\"\n",
    "browser.visit(url_main+url_mars)\n",
    "\n",
    "# use beautifulsoup and its html.parser on html\n",
    "html = browser.html\n",
    "soup = BeautifulSoup(html, 'html.parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/spaceimages/details.php?id=PIA16884'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# scrape the ('url_main'+'url_mars') to find content that includes the link for original image news\n",
    "url_middle = soup.find_all('a', class_='button')\n",
    "\n",
    "# extract the link for the image news from the contents above\n",
    "middle_img_url = url_middle[0].get('data-link')\n",
    "middle_img_url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "## combine the link with 'url_main' to navigate to the image news\n",
    "browser.visit(url_main + middle_img_url)\n",
    "\n",
    "# use beautifulsoup and its html.parser on html\n",
    "html = browser.html\n",
    "soup = BeautifulSoup(html, 'html.parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'https://www.jpl.nasa.gov/spaceimages/images/largesize/PIA16884_hires.jpg'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# scrape the full image link from the image news page\n",
    "img_url = soup.find('figure',class_='lede').a.get('href')\n",
    "\n",
    "# combine the 'img-url' with 'url_main' and store as the featured_image_url\n",
    "featured_image_url = url_main + img_url\n",
    "featured_image_url"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Scrape the weather"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dependencies\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# import the splinter to browse continuous pages\n",
    "from splinter import Browser\n",
    "\n",
    "# https://splinter.readthedocs.io/en/latest/drivers/chrome.html\n",
    "!which chromedriver\n",
    "\n",
    "# use the path to start the chrome\n",
    "executable_path = {'executable_path': '/usr/local/bin/chromedriver'}\n",
    "browser = Browser('chrome', **executable_path, headless=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use Splinter with BeautifulSoup\n",
    "url_tweets = 'https://twitter.com/marswxreport?lang=en'\n",
    "browser.visit(url_tweets)\n",
    "\n",
    "# use beautifulsoup and its html.parser on html\n",
    "html = browser.html\n",
    "soup = BeautifulSoup(html, 'html.parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'InSight sol 92 (2019-03-01) low -94.4ºC (-137.9ºF) high -12.9ºC (8.8ºF)\\nwinds from the SW at 4.6 m/s (10.2 mph) gusting to 10.4 m/s (23.2 mph)\\npressure at 7.20 hPa'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# find contents of weather\n",
    "weather_content = soup.find('p', class_='TweetTextSize TweetTextSize--normal js-tweet-text tweet-text')\n",
    "\n",
    "# only scrape the text in parent level not including the children/embedding <a>\n",
    "# and store the text\n",
    "mars_weather = weather_content.find(text=True, recursive=False)\n",
    "mars_weather"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Scrape Mars facts - table with pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the splinter to browse continuous pages\n",
    "from splinter import Browser\n",
    "\n",
    "# https://splinter.readthedocs.io/en/latest/drivers/chrome.html\n",
    "!which chromedriver\n",
    "\n",
    "# use the path to start the chrome\n",
    "executable_path = {'executable_path': '/usr/local/bin/chromedriver'}\n",
    "browser = Browser('chrome', **executable_path, headless=False)\n",
    "\n",
    "# use pandas to scrape the table\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use Splinter with BeautifulSoup\n",
    "mars_facts = 'http://space-facts.com/mars/'\n",
    "browser.visit(mars_facts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pandas extract table from html\n",
    "tables = pd.read_html(mars_facts) # Returns list of all tables on page\n",
    "mars_extract = tables[0] # Select table of interest\n",
    "\n",
    "# use pandas module to convert dataframework into a '.html' file\n",
    "mars_extract.to_html(\"mars_table.html\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Scrape image url and titles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dependencies\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# import the splinter to browse continuous pages\n",
    "from splinter import Browser\n",
    "\n",
    "# https://splinter.readthedocs.io/en/latest/drivers/chrome.html\n",
    "!which chromedriver\n",
    "\n",
    "# use the path to start the chrome\n",
    "executable_path = {'executable_path': '/usr/local/bin/chromedriver'}\n",
    "browser = Browser('chrome', **executable_path, headless=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use Splinter with BeautifulSoup\n",
    "url_astro_main = 'https://astrogeology.usgs.gov'\n",
    "url_astro_each = '/search/results?q=hemisphere+enhanced&k1=target&v1=Mars'\n",
    "browser.visit(url_astro_main + url_astro_each)\n",
    "\n",
    "# use beautifulsoup and its html.parser on html\n",
    "html = browser.html\n",
    "soup = BeautifulSoup(html, 'html.parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a loop to extract all the img_url and the corresponding titles\n",
    "hemisphere_image_urls = []\n",
    "\n",
    "for i in soup.find('div', class_='collapsible results').find_all('div', class_='item'):\n",
    "    middle = i.a.get('href')\n",
    "    \n",
    "    ## combine the links ('url_astro_main'+'url_astro_each_img_news') to navigate to the image news\n",
    "    browser.visit(url_astro_main + middle)\n",
    "\n",
    "    ## use beautifulsoup and its html.parser on html\n",
    "    html = browser.html\n",
    "    soup = BeautifulSoup(html, 'html.parser')\n",
    "    \n",
    "    # scrape the full img_url and its title\n",
    "    img_url_half = soup.find('img',class_='wide-image').get('src')\n",
    "    \n",
    "    # create a collection for each scraped result\n",
    "    result = {}\n",
    "    result[\"title\"] = soup.find('h2',class_='title').text\n",
    "    result[\"img_url\"] = url_astro_main + img_url_half\n",
    "    hemisphere_image_urls.append(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'img_url': 'https://astrogeology.usgs.gov/cache/images/cfa62af2557222a02478f1fcd781d445_cerberus_enhanced.tif_full.jpg',\n",
      "  'title': 'Cerberus Hemisphere Enhanced'},\n",
      " {'img_url': 'https://astrogeology.usgs.gov/cache/images/3cdd1cbf5e0813bba925c9030d13b62e_schiaparelli_enhanced.tif_full.jpg',\n",
      "  'title': 'Schiaparelli Hemisphere Enhanced'},\n",
      " {'img_url': 'https://astrogeology.usgs.gov/cache/images/ae209b4e408bb6c3e67b6af38168cf28_syrtis_major_enhanced.tif_full.jpg',\n",
      "  'title': 'Syrtis Major Hemisphere Enhanced'},\n",
      " {'img_url': 'https://astrogeology.usgs.gov/cache/images/7cf2da4bf549ed01c17f206327be4db7_valles_marineris_enhanced.tif_full.jpg',\n",
      "  'title': 'Valles Marineris Hemisphere Enhanced'}]\n"
     ]
    }
   ],
   "source": [
    "from pprint import pprint\n",
    "\n",
    "## pretty print the final json\n",
    "pprint(hemisphere_image_urls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
